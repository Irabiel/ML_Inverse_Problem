{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8189f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.w1 = np.random.randn(2, 4)\n",
    "        self.w2 = np.random.randn(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = 1 / (1 + np.exp(-np.dot(x, self.w1)))\n",
    "        y_pred = 1 / (1 + np.exp(-np.dot(h, self.w2)))\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = np.mean((y - y_pred)**2)\n",
    "        return loss\n",
    "\n",
    "    def parameters(self):\n",
    "        return np.concatenate([self.w1.ravel(), self.w2.ravel()])\n",
    "\n",
    "    def set_parameters(self, params):\n",
    "        self.w1 = np.reshape(params[:8], (2, 4))\n",
    "        self.w2 = np.reshape(params[8:], (4, 1))\n",
    "\n",
    "# Define the training data\n",
    "x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Instantiate the neural network\n",
    "net = Net()\n",
    "\n",
    "# Define the optimization function\n",
    "def optimize(params):\n",
    "    net.set_parameters(params)\n",
    "    return net.loss(x_train, y_train)\n",
    "\n",
    "# Train the neural network using scipy.optimize.minimize\n",
    "res = minimize(optimize, net.parameters(), method='BFGS', options={'maxiter': 500, 'disp': True})\n",
    "\n",
    "# Print the results\n",
    "print(f\"Optimization status: {res.message}\")\n",
    "print(f\"Final loss: {res.fun}\")\n",
    "print(f\"Optimized parameters: {res.x}\")\n",
    "\n",
    "# Evaluate the neural network on the test data\n",
    "x_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_test = np.array([0, 1, 1, 0])\n",
    "y_pred = np.round(net.forward(x_test)).astype(int)\n",
    "print(f\"Test accuracy: {(y_pred == y_test).sum() / len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc07764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, 10)\n",
    "        self.fc2 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the dataset\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_data = torch.Tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Initialize the neural network model\n",
    "net = Net()\n",
    "\n",
    "# Define the optimizer\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = net(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    loss.backward()\n",
    "    return loss.item()\n",
    "\n",
    "optimizer = torch.optim.LBFGS(net.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "max_iter = 100\n",
    "for i in range(max_iter):\n",
    "    loss = optimizer.step(closure)\n",
    "    print(f\"iteration {i+1}: loss = {loss:.4f}\")\n",
    "\n",
    "# Print the learned parameters\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.5, random_state=1)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Define the neural network\n",
    "net = torch.nn.Sequential(torch.nn.Linear(5, 10),\n",
    "                           torch.nn.ReLU(),\n",
    "                           torch.nn.Linear(10, 1))\n",
    "\n",
    "# Define the loss function\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Flatten the neural network parameters\n",
    "n_params = sum(p.numel() for p in net.parameters())\n",
    "params_init = torch.randn(n_params)\n",
    "\n",
    "# Define the loss function\n",
    "def loss(params):\n",
    "    # Load the current set of parameters into the neural network\n",
    "    with torch.no_grad():\n",
    "        net_params = torch.tensor(params, dtype=torch.float32).reshape(-1, 1)\n",
    "        current_param_idx = 0\n",
    "        for p in net.parameters():\n",
    "            p.data = net_params[current_param_idx:current_param_idx+p.numel()].reshape(p.shape)\n",
    "            current_param_idx += p.numel()\n",
    "\n",
    "    # Compute the output of the neural network for the given input\n",
    "    y_pred = net(x_tensor)\n",
    "\n",
    "    # Compute the mean squared error loss between the predicted and actual outputs\n",
    "    loss = mse_loss(y_tensor, y_pred)\n",
    "    return loss.item()\n",
    "\n",
    "# Use L-BFGS-B method to optimize the parameters\n",
    "options = {\"maxiter\": 100, \"disp\": True}\n",
    "res = minimize(loss, params_init, method=\"BFGS\", options=options)\n",
    "\n",
    "# Load the optimized parameters back to the neural network\n",
    "with torch.no_grad():\n",
    "    net_params = torch.tensor(res.x, dtype=torch.float32).reshape(-1, 1)\n",
    "    current_param_idx = 0\n",
    "    for p in net.parameters():\n",
    "        p.data = net_params[current_param_idx:current_param_idx+p.numel()].reshape(p.shape)\n",
    "        current_param_idx += p.numel()\n",
    "\n",
    "# Compute the final loss\n",
    "final_loss = loss(res.x)\n",
    "print(f\"Final loss: {final_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "net = Net()\n",
    "\n",
    "# Create some input data\n",
    "x = torch.randn(10, 1)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define a function to compute the loss and gradients\n",
    "def fun(params):\n",
    "    # Copy the parameters to the neural network\n",
    "    with torch.no_grad():\n",
    "        for p, param in zip(net.parameters(), params):\n",
    "            p.copy_(torch.tensor(param))\n",
    "\n",
    "    # Compute the output of the neural network\n",
    "    output = net(x)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(output, torch.ones_like(output))\n",
    "\n",
    "    # Compute the gradients\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    gradients = []\n",
    "    for p in net.parameters():\n",
    "        gradients.append(p.grad.view(-1).numpy())\n",
    "\n",
    "    # Return the loss and gradients\n",
    "    return loss.item(), np.concatenate(gradients)\n",
    "\n",
    "# Initialize the parameters\n",
    "params0 = np.concatenate([p.view(-1).detach().numpy() for p in net.parameters()])\n",
    "\n",
    "\n",
    "# Use Scipy to optimize the parameters\n",
    "res = minimize(fun, params0, jac=True, method='BFGS')\n",
    "\n",
    "# Copy the optimized parameters to the neural network\n",
    "with torch.no_grad():\n",
    "    for p, param in zip(net.parameters(), res.x):\n",
    "        p.copy_(torch.tensor(param))\n",
    "\n",
    "# Compute the output of the neural network\n",
    "output = net(x)\n",
    "\n",
    "# Backpropagate the gradients\n",
    "output.backward(torch.ones_like(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16993db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.dtype)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "def loss_fn(y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Define the training data\n",
    "x_train = torch.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "y_train = x_train ** 2\n",
    "\n",
    "# Initialize the neural network\n",
    "net = Net()\n",
    "\n",
    "# Define the training loop\n",
    "def train(params):\n",
    "    # Update the neural network weights\n",
    "    with torch.no_grad():\n",
    "#         print(net.fc.weight.dtype)\n",
    "        net.fc.weight = torch.nn.Parameter(torch.tensor(params).reshape(1, 1), requires_grad=True)\n",
    "        net.double()\n",
    "#         print(net.fc.weight.dtype)\n",
    "        \n",
    "    # Compute the predictions and loss\n",
    "    y_pred = net(x_train.double())\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    # Compute the gradients and update the weights\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    grads = net.fc.weight.grad.detach().numpy().reshape(1)\n",
    "    grads = grads.astype('float64') # convert to the correct data type\n",
    "    return loss.item(), grads\n",
    "\n",
    "# Define the optimizer\n",
    "def optimize(params):\n",
    "    loss, grads = train(params)\n",
    "    return loss, grads\n",
    "\n",
    "# Initialize the weight parameter and optimize it with scipy\n",
    "params0 = net.fc.weight.detach().numpy().reshape(1)\n",
    "bounds = [(None, None)]\n",
    "for epoch in range(100):\n",
    "    result = minimize(optimize, params0, jac=True, method='L-BFGS-B', bounds=bounds)\n",
    "    params0 = result.x\n",
    "\n",
    "# print(result)\n",
    "\n",
    "# Update the neural network weights with the optimized value\n",
    "net.fc.weight = torch.nn.Parameter(torch.tensor(result.x).reshape(1, 1))\n",
    "\n",
    "# Test the neural network on some new data\n",
    "x_test = torch.tensor([[2.0], [3.0], [4.0]])\n",
    "y_pred = net(x_test.double())\n",
    "print(x_test, y_pred, params0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09394625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_fn(output, target):\n",
    "    return torch.mean((output - target) ** 2)\n",
    "\n",
    "# Create an instance of the neural network\n",
    "net = Net()\n",
    "\n",
    "# Define the input tensor\n",
    "x = torch.tensor([[1.0]])\n",
    "\n",
    "# Define the target tensor\n",
    "target = x ** 2\n",
    "\n",
    "# Define the optimization function\n",
    "def optimize_fn(x):\n",
    "    net.linear.weight.data = torch.tensor([[x]])\n",
    "    input_tensor = torch.tensor([[x]])\n",
    "    output = net(input_tensor)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward(torch.ones_like(output))\n",
    "    return loss.item()\n",
    "\n",
    "# Train the neural network using Scipy's minimize_scalar function\n",
    "res = minimize_scalar(optimize_fn)\n",
    "print(\"Optimal weight:\", res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c46db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
